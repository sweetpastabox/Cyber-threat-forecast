{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math\n",
    "from numpy import split\n",
    "import random\n",
    "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\n",
    "from scipy.optimize import minimize\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import numpy\n",
    "from tqdm import tqdm_notebook\n",
    "from keras.callbacks import EarlyStopping\n",
    "from numpy import array\n",
    "#from itertools import product\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from numpy.random import seed\n",
    "from scipy import stats\n",
    "from IPython.display import clear_output \n",
    "import statistics\n",
    "import keras.backend as K\n",
    "\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "\n",
    "\n",
    "# evaluation mertic (symmteric mean absolute percentage error)\n",
    "def smape(yTrue,yPred):\n",
    "  smape=0\n",
    "  \n",
    "  for i in range(len(yTrue)):\n",
    "    smape+= abs(yTrue[i]-yPred[i])/ (abs(yTrue[i])+abs(yPred[i]))\n",
    "  smape/=len(yTrue)\n",
    "\n",
    "  return smape\n",
    "\n",
    "\n",
    "def maximum(a, b):\n",
    "      \n",
    "    if a >= b:\n",
    "        return a\n",
    "    else:\n",
    "        return b\n",
    "\n",
    "def minimum(a, b):\n",
    "      \n",
    "    if a <= b:\n",
    "        return a\n",
    "    else:\n",
    "        return b\n",
    "\n",
    "\n",
    "\n",
    "def exponential_smoothing(series, alpha):\n",
    "\n",
    "    result = [series[0]] # first value is same as series\n",
    "    for n in range(1, len(series)):\n",
    "        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n",
    "    return result\n",
    "  \n",
    "def plot_exponential_smoothing(series, alphas):\n",
    " \n",
    "    plt.figure(figsize=(17, 8))\n",
    "    for alpha in alphas:\n",
    "        plt.plot(exponential_smoothing(series, alpha), label=\"Alpha {}\".format(alpha))\n",
    "    plt.plot(series.values, \"c\", label = \"Actual\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.axis('tight')\n",
    "    plt.title(\"Exponential Smoothing - \"+attack)\n",
    "    plt.grid(True);\n",
    "\n",
    "def double_exponential_smoothing(series, alpha, beta):\n",
    "\n",
    "    result = [series[0]]\n",
    "    for n in range(1, len(series)+1):\n",
    "        if n == 1:\n",
    "            level, trend = series[0], series[1] - series[0]\n",
    "        if n >= len(series): # forecasting\n",
    "            value = result[-1]\n",
    "        else:\n",
    "            value = series[n]\n",
    "        last_level, level = level, alpha * value + (1 - alpha) * (level + trend)\n",
    "        trend = beta * (level - last_level) + (1 - beta) * trend\n",
    "        result.append(level + trend)\n",
    "    return result\n",
    "\n",
    "def plot_double_exponential_smoothing(series, alphas, betas):\n",
    "     \n",
    "    plt.figure(figsize=(17, 8))\n",
    "    for alpha in alphas:\n",
    "        for beta in betas:\n",
    "            plt.plot(double_exponential_smoothing(series, alpha, beta), label=\"Alpha {}, beta {}\".format(alpha, beta))\n",
    "    plt.plot(series.values, label = \"Actual\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.axis('tight')\n",
    "    plt.title(\"Double Exponential Smoothing - \"+attack)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "#stationarity test\n",
    "def tsplot(y, lags=None, figsize=(12, 7), syle='bmh'):\n",
    "    \n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "        \n",
    "    with plt.style.context(style='bmh'):\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        layout = (2,2)\n",
    "        ts_ax = plt.subplot2grid(layout, (0,0), colspan=2)\n",
    "        acf_ax = plt.subplot2grid(layout, (1,0))\n",
    "        pacf_ax = plt.subplot2grid(layout, (1,1))\n",
    "        \n",
    "        y.plot(ax=ts_ax)\n",
    "        p_value = sm.tsa.stattools.adfuller(y)[1]\n",
    "        ts_ax.set_title(attack_title+' - Time Series Analysis Plots\\n Dickey-Fuller: p={0:.5f}'.format(p_value))\n",
    "        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n",
    "        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    " \n",
    "# convert history into inputs and outputs\n",
    "def to_supervised(train, n_input, n_out=1):\n",
    "    # flatten data\n",
    "    data = train #modified\n",
    "\n",
    "    X, y = list(), list()\n",
    "    in_start = 0\n",
    "    # step over the entire history one time step at a time\n",
    "    for _ in range(len(data)):\n",
    "        # define the end of the input sequence\n",
    "        in_end = in_start + n_input\n",
    "        out_end = in_end + n_out\n",
    "        # ensure we have enough data for this instance\n",
    "        if out_end <= len(data):\n",
    "            X.append(data[in_start:in_end, :])\n",
    "            y.append(data[in_end:out_end, 0])\n",
    "        # move along one time step\n",
    "        in_start += 1\n",
    "    return array(X), array(y)\n",
    " \n",
    "#Bayesian LSTM\n",
    "class BayesianLSTM(LSTM): #inherits LSTM but set training=True which keeps dropout on during test time.\n",
    "  def call(self, inputs):\n",
    "    return super().call(inputs, training=True)\n",
    "\n",
    "# train the model using Bayesian LSTM\n",
    "def build_model(train, test, n_input, layer, unit, epoch, lr,rdo):\n",
    "    # prepare data\n",
    "    train_x, train_y = to_supervised(train, n_input)\n",
    "    test_x, test_y=to_supervised(test,n_input)\n",
    " \n",
    "    # define parameters\n",
    "    verbose, epochs, batch_size = 0,epoch, 8\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    if layer==1:\n",
    "        model.add(BayesianLSTM(unit[0], input_shape=(n_input, 1),recurrent_dropout=rdo, activation = 'relu'))\n",
    "        model.add(RepeatVector(1))\n",
    "        model.add(BayesianLSTM(unit[0], activation = 'relu', return_sequences=True, recurrent_dropout=rdo))\n",
    "        model.add(TimeDistributed(Dense(100, activation = 'relu')))\n",
    "        model.add(TimeDistributed(Dense(1)))\n",
    "    elif layer==2:\n",
    "        model.add(BayesianLSTM(unit[0], activation = 'relu', input_shape=(n_input, 1),return_sequences=True,recurrent_dropout=rdo))\n",
    "        model.add(BayesianLSTM(unit[1], activation = 'relu' ,recurrent_dropout=rdo))\n",
    "        model.add(RepeatVector(1))\n",
    "        model.add(BayesianLSTM(unit[1], activation = 'relu', return_sequences=True,recurrent_dropout=rdo))\n",
    "        model.add(BayesianLSTM(unit[0], activation = 'relu', return_sequences=True,recurrent_dropout=rdo))\n",
    "        model.add(TimeDistributed(Dense(100, activation = 'relu')))\n",
    "        model.add(TimeDistributed(Dense(1)))\n",
    "    elif layer==3:\n",
    "        model.add(BayesianLSTM(unit[0], activation = 'relu', input_shape=(n_input, 1),return_sequences=True,recurrent_dropout=rdo))\n",
    "        model.add(BayesianLSTM(unit[1], activation = 'relu' ,return_sequences=True,recurrent_dropout=rdo))\n",
    "        model.add(BayesianLSTM(unit[2], activation = 'relu' ,recurrent_dropout=rdo))\n",
    "        model.add(RepeatVector(1))\n",
    "        model.add(BayesianLSTM(unit[2], activation = 'relu', return_sequences=True,recurrent_dropout=rdo))\n",
    "        model.add(BayesianLSTM(unit[1], activation = 'relu', return_sequences=True,recurrent_dropout=rdo))\n",
    "        model.add(BayesianLSTM(unit[0], activation = 'relu', return_sequences=True,recurrent_dropout=rdo))\n",
    "        model.add(TimeDistributed(Dense(100, activation = 'relu')))\n",
    "        model.add(TimeDistributed(Dense(1)))\n",
    "          \n",
    "\n",
    "    opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=opt, loss='mse', metrics=['mape']) #compile the model with mean square error as the loss function\n",
    "    \n",
    "    #if the loss does not improve after 30 epochs, stop anyway (early stopping)\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=30)\n",
    "\n",
    "    # fit network\n",
    "    train_history= model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, validation_data=(test_x, test_y),verbose=verbose,callbacks=[es])\n",
    "    \n",
    "    loss = train_history.history['loss']\n",
    "    val_loss = train_history.history['val_loss']\n",
    "\n",
    "    return model\n",
    " \n",
    " \n",
    "# evaluate a single model\n",
    "def evaluate_model(train, test, n_input,  alpha, beta,lag,layer, unit, lr, epoch,rdo, attack,label,seed):\n",
    "    \n",
    "  n_iterations=10\n",
    "\n",
    "  # fit model\n",
    "  model = build_model(train, test, n_input, layer, unit, epoch, lr,rdo)\n",
    "  # history is a list of n_input size data\n",
    "\n",
    "  predictions_2d = list()\n",
    "\n",
    "  #Bayesian LSTM performs multiple iterations to create a prediction distribution (i.e., mean and variance)\n",
    "  for r in range(0,n_iterations):\n",
    "    predictions = list()\n",
    "\n",
    "    #look back in test data\n",
    "    history=test[0:n_input,]\n",
    "\n",
    "    #sliding window prediction (3 years in advance=36 iterations)\n",
    "    for z in test[n_input:,0]:\n",
    "\n",
    "        y_hat=model.predict(history.reshape(1,history.shape[0],history.shape[1]))\n",
    "        history=numpy.append(history, y_hat[0][0])\n",
    "        history=history.reshape(history.shape[0],1)\n",
    "        history=history[1:,]#move the sliding window 1 step\n",
    "        predictions.append(y_hat[0][0])\n",
    "    predictions_2d.append(predictions)\n",
    "\n",
    "\n",
    "\n",
    "  predictions_2d = array(predictions_2d)\n",
    "  mean_predictions=numpy.mean(predictions_2d,axis=0) #mean \n",
    "  var_predictions=numpy.var(predictions_2d,axis=0) #variance\n",
    "  std_predictions=numpy.std(predictions_2d,axis=0) #standard deviation\n",
    "  z=1.96\n",
    "  confidence_95=z*std_predictions/math.sqrt(len(mean_predictions))# 95% confidence interval\n",
    "\n",
    "  #calculate the height of data on the plot (plot range) to disable variance/95% conf.interval plot if too large or too small.\n",
    "  upper=maximum(numpy.amax(mean_predictions),numpy.amax(test[n_input:,0]))\n",
    "  lower=minimum(numpy.amin(mean_predictions),numpy.amin(test[n_input:,0]))\n",
    "  p_range=abs(upper-lower)\n",
    "  var_plot=True\n",
    "  max_var=0\n",
    "  for v in var_predictions:\n",
    "    if v>p_range:\n",
    "      print('disabling variance plot due to large variance=',v)\n",
    "      var_plot=False\n",
    "    if v>max_var:\n",
    "      max_var=v\n",
    "  if max_var*30<p_range:\n",
    "    print('disabling variance plot due to small max variance=',max_var)\n",
    "    var_plot=False\n",
    "\n",
    "  conf_plot=True\n",
    "  max_conf=0\n",
    "  for conf in confidence_95:\n",
    "    if conf>p_range:\n",
    "      print('disabling confidence plot due to large confidence=',conf)\n",
    "      conf_plot=False\n",
    "    if conf>max_conf:\n",
    "      max_conf=conf\n",
    "  if max_conf*30<p_range:\n",
    "    print('disabling confidence plot due to small max confidence=',max_conf)\n",
    "    conf_plot=False\n",
    "\n",
    "\n",
    "  actual=test[n_input:,]\n",
    "  mape= smape(actual,mean_predictions) #calculate the error as the symmteric mean absolute percentage error (SMAPE)\n",
    "\n",
    "  #penalizing small range (M-SMAPE)\n",
    "  range_a= numpy.amax(actual)-numpy.amin(actual)\n",
    "  range_p= numpy.amax(mean_predictions)-numpy.amin(mean_predictions)\n",
    "  if(range_p<range_a/3):\n",
    "    mape+=0.3\n",
    "    print('**** PENALIZED: +0.3 ******')\n",
    "  \n",
    "  #record hyperparameters\n",
    "  hp=[alpha, beta,lag,layer, unit, lr, epoch,rdo,label]\n",
    "  print('Hyperparams=',hp)\n",
    "  print('MAPE=',mape)\n",
    " \n",
    "  \n",
    "  #create a plot of actual vs predicted\n",
    "  months=['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec']\n",
    "  M=[]\n",
    "  for year in range (11,23):   \n",
    "      for month in months:\n",
    "        if year==11 and month not in ['Jul','Aug','Sep','Oct','Nov','Dec']:\n",
    "          continue\n",
    "        if year==22 and month not in ['Jan','Feb','Mar']:\n",
    "          continue;\n",
    "        M.append(month+'-'+str(year))       \n",
    "\n",
    "  \n",
    "  M=M[-len(actual):]\n",
    "  M2=[]\n",
    "  p=[]\n",
    "  for index,value in enumerate(M):\n",
    "    if 'Dec' in M[index] or 'Mar' in M[index] or 'Jun' in M[index] or 'Sep' in M[index]:\n",
    "      M2.append(M[index])\n",
    "      p.append(index+1)\n",
    "\n",
    "    \n",
    "  x=range(1,len(mean_predictions)+1)\n",
    "  pyplot.plot(x,actual,'b-',label='Actual')\n",
    "  pyplot.plot(x,mean_predictions,'--', color=\"orange\",label='Predicted')\n",
    "  if conf_plot:\n",
    "    pyplot.fill_between(x,(mean_predictions.reshape(-1)- (confidence_95.reshape(-1))), (mean_predictions.reshape(-1)+confidence_95.reshape(-1)), color='green', alpha=0.2,label='95% Confidence')\n",
    "  if var_plot:\n",
    "    pyplot.fill_between(x,(mean_predictions.reshape(-1)- (var_predictions.reshape(-1))), (mean_predictions.reshape(-1)+var_predictions.reshape(-1)), color='green', alpha=0.1,label='Variance')\n",
    "  plt.legend(loc=\"best\",prop={'size': 11})\n",
    "  plt.axis('tight')\n",
    "  plt.grid(True)\n",
    "  pyplot.title(attack+' (U)', y=1.03,fontsize=18)\n",
    "  #pyplot.xlabel(\"Month\")\n",
    "  pyplot.ylabel(\"No of incidents\",fontsize=15)\n",
    "  locs, labs = plt.xticks() \n",
    "  plt.xticks(ticks = p ,labels = M2, rotation='vertical',fontsize=13) \n",
    "  plt.yticks(fontsize=13)\n",
    "  fig = pyplot.gcf()\n",
    "\n",
    "  #save the results (plot and hyperparameter values) if the error is below certain threshold. \n",
    "  if mape<=0.15:  \n",
    "    images_dir = 'output_validation_results/'\n",
    "    plt.savefig(f\"{images_dir}/\"+attack.replace('/','_')+'_t_'+str(seed)+\".png\", bbox_inches = \"tight\")\n",
    "    with open('output_validation_results/'+attack_title.replace('/','_')+'_t_'+str(seed)+'.txt', 'w') as f: #the name of the file includes the seed\n",
    "      f.write('MAPE: '+str(mape))\n",
    "      f.write('\\nHyperparams: '+str(hp))\n",
    "      f.close()\n",
    "\n",
    "\n",
    "  pyplot.show()\n",
    "\n",
    "\n",
    "  return model,mape\n",
    "\n",
    "\n",
    "#forecasts the future trend of a single attack 3 years in advance\n",
    "def forecast(data, e_data, model, min_list,seed, attack_label):\n",
    "    \n",
    "\n",
    "    #get best model's hyperparameters\n",
    "    n_input=min_list[2]\n",
    "    layer=min_list[3]\n",
    "    unit=min_list[4]\n",
    "    lr=min_list[5]\n",
    "    epoch=min_list[6]\n",
    "    rdo=min_list[7]\n",
    "\n",
    "    #build the model using the best hyperparameters using the whole data\n",
    "    model = build_model(e_data, e_data, n_input, layer, unit, epoch, lr,rdo)\n",
    "\n",
    "\n",
    "    #Bayesian LSTM performs multiple iterations to create a prediction distribution (i.e., mean and variance)\n",
    "    n_iterations=10\n",
    "    predictions_2d=list()\n",
    "    for r in range(0,n_iterations):\n",
    "\n",
    "      predictions = list()\n",
    "      predictions.append(e_data[-1,0]) #for plotting purpose\n",
    "      #look back in test data\n",
    "      history=e_data[-n_input:,]\n",
    "\n",
    "      #predicts 36 months using sliding window\n",
    "      for i in range(36): \n",
    "        f_hat=model.predict(history.reshape(1,history.shape[0],history.shape[1]))\n",
    "        history=numpy.append(history, f_hat[0][0])\n",
    "        history=history.reshape(history.shape[0],1)\n",
    "        history=history[1:,]#move the sliding window 1 step\n",
    "        predictions.append(f_hat[0][0][0])\n",
    "      predictions_2d.append(predictions)\n",
    "\n",
    "    predictions_2d = array(predictions_2d)\n",
    "    predictions_mean=numpy.mean(predictions_2d,axis=0) #mean\n",
    "    predictions_var=numpy.var(predictions_2d,axis=0) #variance\n",
    "    std_predictions=numpy.std(predictions_2d,axis=0) #standard deviation\n",
    "    z=1.96\n",
    "    confidence_95=z*std_predictions/math.sqrt(len(predictions_mean))# 95% confidence interval\n",
    "\n",
    "    #calculate the height of data on the plot (plot range) to disable variance/95% conf.interval plot if too large or too small.\n",
    "    upper=maximum(numpy.amax(predictions_mean),numpy.amax(data))\n",
    "    lower=minimum(numpy.amin(predictions_mean),numpy.amin(data))\n",
    "    p_range=upper-lower\n",
    "    var_plot=True\n",
    "    max_var=0\n",
    "    for v in predictions_var:\n",
    "      if v>p_range:\n",
    "        var_plot=False\n",
    "      if v>max_var:\n",
    "        max_var=v\n",
    "    if max_var*30<p_range:\n",
    "      var_plot=False\n",
    "\n",
    "    conf_plot=True\n",
    "    max_conf=0\n",
    "    for conf in confidence_95:\n",
    "      if conf>p_range:\n",
    "        conf_plot=False\n",
    "      if conf>max_conf:\n",
    "        max_conf=conf\n",
    "    if max_conf*30<p_range:\n",
    "      conf_plot=False \n",
    "\n",
    "\n",
    "    print(e_data)\n",
    "    print(predictions_mean)\n",
    "    \n",
    "    #Plot the forecast\n",
    "    x=['2012', '2013','2014', '2015', '2016', '2017', '2018','2019', '2020', '2021','2022','2023','2024','2025']\n",
    "    pyplot.plot(range(len(data)),data,'--', color=\"blue\",label='Data')\n",
    "    pyplot.plot(range(len(data), len(data)+len(predictions_mean)),predictions_mean,'--', color=\"red\",label='Prediction')\n",
    "    if conf_plot:\n",
    "      pyplot.fill_between(range(len(data), len(data)+len(predictions_mean)),(predictions_mean.reshape(-1)- (confidence_95.reshape(-1))), (predictions_mean.reshape(-1)+confidence_95.reshape(-1)), color='green', alpha=0.2,label='95% Confidence')\n",
    "    if var_plot:\n",
    "      pyplot.fill_between(range(len(data), len(data)+len(predictions_mean)),(predictions_mean.reshape(-1)- (predictions_var.reshape(-1))), (predictions_mean.reshape(-1)+predictions_var.reshape(-1)), color='green', alpha=0.1,label='Variance')\n",
    "    plt.xticks([6,18,30,42,54,66,78,90,102,114,126,138,150,162], x,fontsize=13) # positions of years on x axis\n",
    "\n",
    "    plt.ylabel(\"No of incidents\",fontsize=15)\n",
    "    plt.legend(loc=\"best\",prop={'size': 11})\n",
    "    plt.axis('tight')\n",
    "    plt.grid(True)\n",
    "    pyplot.xticks(rotation=90)\n",
    "    pyplot.title(attack_label+' (U)', y=1.03,fontsize=18)\n",
    "    locs, labs = plt.xticks() \n",
    "    plt.yticks(fontsize=13)\n",
    "     \n",
    "    #save the forecast plot to file\n",
    "    images_dir = 'output_hyperparameters/'\n",
    "    plt.savefig(f\"{images_dir}/\"+attack_label.replace('/','_')+str(seed)+\".png\", bbox_inches = \"tight\")#the name of the file includes the seed\n",
    "\n",
    "    pyplot.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#number of iterations of random search for hp optimisation\n",
    "iterations=60\n",
    "#read input data\n",
    "data = read_csv('input_data/T-HMGDN-F-0711-0322.csv', header=0)\n",
    "attacks=data.columns.values.tolist()\n",
    "raw_values = data.values\n",
    "avg_data=0\n",
    "\n",
    "\n",
    "#name of attack columns (in the dataset). All refers to the number of attacks in all countries combined \n",
    "attacks=['DDoS-ALL', 'Phishing-ALL', 'Ransomware-ALL', 'Password Attack-ALL', 'SQL Injection-ALL', 'Account Hijacking-ALL', \n",
    "         'Defacement-ALL', 'Trojan-ALL', 'Vulnerability-ALL', 'Zero-day-ALL', 'Malware-ALL', 'Advanced persistent threat-ALL', \n",
    "         'XSS-ALL', 'Data Breach-ALL', 'Disinformation/Misinformation-ALL', 'Targeted Attack-ALL','Adware-ALL',\n",
    "         'Brute Force Attack-ALL', 'Malvertising-ALL', 'Backdoor-ALL', 'Botnet-ALL', 'Cryptojacking-ALL',\n",
    "         'Worms-ALL', 'Spyware-ALL', 'Mentions-MITM', 'Mentions-DNS Spoofing', \n",
    "         'Mentions-WannaCry Ransomware','Mentions-Dropper', 'Mentions-Wiper', 'Mentions-Pharming', 'Mentions-Insider Threat',\n",
    "         'Mentions-Drive-by', 'Mentions-Rootkit', 'Mentions-Adversarial Attack', 'Mentions-Data Poisoning', 'Mentions-Deepfake',\n",
    "         'Mentions-Supply Chain', 'Mentions-IoT Device Attack', 'Mentions-Keylogger', 'Mentions-DNS Tunneling', 'Mentions-Session Hijacking',\n",
    "         'Mentions-URL manipulation']\n",
    "done=[] #can be manually appended with the completed attacks (to avoid repeating previously optimised attacks)\n",
    "\n",
    "\n",
    "#iterate over all attacks to optimise the model of each\n",
    "for attack in attacks:\n",
    "  clear_output()\n",
    "  if attack in done:\n",
    "    continue\n",
    "  if not 'ALL' in attack and not 'Mentions' in attack:\n",
    "    continue;\n",
    "  if 'Mentions-' in attack:\n",
    "    attack_title=attack[attack.index('-')+1:]\n",
    "  else:\n",
    "    attack_title=attack[0:attack.rindex('-')]\n",
    "\n",
    "\n",
    "  if len(attack_title)>18:\n",
    "    attack_title=attack_title.rsplit(' ', 1)[0]\n",
    "\n",
    " \n",
    "  best_data=None\n",
    "  best_e_data=None\n",
    "   \n",
    "\n",
    "  min_mape=999999999\n",
    "  min_list=[]\n",
    "  model=None\n",
    "  c=1 #seed\n",
    "\n",
    "\n",
    "  #alpha and beta are smoothing constants and part of hyperparameters optimisation \n",
    "  alphas_=[0.05,0.2,0.5,0.7,1]\n",
    "  betas_=[0.3,0.5,0.7,1]\n",
    "  #random search for hyperparameters\n",
    "  for _ in range(iterations):\n",
    "    alphas=[alphas_[random.randint(0, len(alphas_)-1)]]\n",
    "    betas=[betas_[random.randint(0, len(betas_)-1)]]\n",
    "    for alpha in alphas:\n",
    "      for beta in betas:\n",
    "\n",
    "        if alpha==1 and not beta==1:\n",
    "          continue;\n",
    "\n",
    "        #1. smoothing    \n",
    "        #if alpha and beta <1 then the method is double exponential smoothing\n",
    "        # if alpha<1 and beta = 1, the method is exponential smoothing\n",
    "        # if both beta and alpha equal 1, then no smoothing\n",
    "        if not alpha==1 and not beta==1:\n",
    "            smoothed=double_exponential_smoothing(data[attack], alpha,beta)\n",
    "            smoothed=smoothed[0:-1] #remove the one future step forecast from des\n",
    "            plt.plot(smoothed, label=\"Alpha {}, beta {}\".format(alpha, beta))\n",
    "            plt.title(\"Double Exponential Smoothing - \"+attack_title)\n",
    "            avg_data=sum(smoothed)/len(smoothed)\n",
    "        elif not alpha==1 and beta==1: \n",
    "            smoothed=exponential_smoothing(data[attack], alpha)\n",
    "            plt.plot(smoothed, label=\"Alpha {}\".format(alpha))\n",
    "            plt.title(\"Exponential Smoothing - \"+attack_title)\n",
    "            avg_data=sum(smoothed)/len(smoothed)\n",
    "        elif alpha==1 and beta==1:\n",
    "            #no smoothing\n",
    "            smoothed=data[attack]\n",
    "            avg_data=smoothed.mean()\n",
    "\n",
    "        #visualise data\n",
    "        plt.plot(data[attack], label = \"Actual\")\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.axis('tight')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(\"Month\")\n",
    "        plt.ylabel(\"No of incidents\")\n",
    "        plt.show()\n",
    "        print('average data is:',avg_data)\n",
    "\n",
    "\n",
    "        #2. stationarity test\n",
    "        tsplot(smoothed, lags=30)\n",
    "\n",
    "\n",
    "        smoothed_all=numpy.array([smoothed]).reshape(-1,1)\n",
    "        prepared_data=smoothed\n",
    "        scaled_data=numpy.array(prepared_data).reshape(-1,1)\n",
    "\n",
    "        #when did incidents start appearing? cut the part of the data earlier than that\n",
    "        e_index=0 #emerging index\n",
    "        for i in range(len(scaled_data)-2):\n",
    "          if scaled_data[i,0]>0 and scaled_data[i+1,0]>0 and scaled_data[i+2,0]>0:\n",
    "            e_index=i\n",
    "            break\n",
    "        print('Emerging Index=',e_index)\n",
    "        scaled_e_data=scaled_data[e_index:]\n",
    "        print('Length of data after cut=',len(scaled_e_data))\n",
    "      \n",
    "\n",
    "        #Model Hyperparameters\n",
    "        lags_=list(range(1,maximum(len(scaled_e_data)//10,1)+1))#range of possible lags to choose from is 1 to length of data/10\n",
    "        lag=lags_[random.randint(0, len(lags_)-1)]\n",
    "        lrs_=[0.001,0.0001,0.0006,0.01] #learning rate\n",
    "        lr=lrs_[random.randint(0, len(lrs_)-1)]\n",
    "        epochs_=[30,50,100,200]\n",
    "        epoch=epochs_[random.randint(0, len(epochs_)-1)]\n",
    "        units_=[[50],[25],[100],[200],[64,28],[40,20],[100,50]]\n",
    "        unit=units_[random.randint(0, len(units_)-1)]\n",
    "        layer=len(unit)\n",
    "        rdos_=[0.2,0.3,0.4,0.5] #recurrent dropout\n",
    "        rdo=rdos_[random.randint(0, len(rdos_)-1)]\n",
    "\n",
    "\n",
    "        repeat=3 #repeat the test for the same set of hyperparameters but with different seeds (to take the average) \n",
    "        avg_mape=0.0  #average error\n",
    "        n_input=lag \n",
    "        #repeat the test for the same set of hyperparameters but with different seeds (to take the average)\n",
    "        for _ in range(0,repeat):\n",
    "            seed(c)\n",
    "            tf.random.set_seed(c)   \n",
    "            diff_label='not differenced' #curve not differenced in this experiment\n",
    "            print('seed:',c,'------> DOING:',alpha, beta,lag,layer, unit, lr, epoch,rdo,diff_label)\n",
    "            c+=1\n",
    "\n",
    "            half=len(scaled_e_data)//2\n",
    "            if half>36+n_input:\n",
    "              split_position=len(scaled_e_data)-(36+n_input)# 3 years + # months look back testing  \n",
    "            else:\n",
    "              split_position=half # if data is small, take half of it (e.g. emerging attacks)\n",
    "\n",
    "            train=scaled_e_data[0:split_position]\n",
    "            validate=scaled_e_data[split_position:]\n",
    "            test=scaled_e_data[split_position:]\n",
    "\n",
    "            print('length of train=',len(train))\n",
    "            print('length of validate=',len(validate))\n",
    "            print('length of test=',len(test))\n",
    "            train=array(train)\n",
    "            validate=array(validate)\n",
    "            test=array(test)\n",
    "\n",
    "            # evaluate model and get scores\n",
    "            model,mape  = evaluate_model(train,validate, n_input, alpha, beta,lag,layer, unit, lr, epoch,rdo, attack_title,diff_label,c-1)\n",
    "            avg_mape=avg_mape+mape\n",
    "            print ('MIN MAPE:',min_mape);\n",
    "            print ('Best Hyperparams:',min_list);\n",
    "\n",
    "        avg_mape/=repeat\n",
    "        #keep record of the lowest error and the best hyperparameters\n",
    "        if(avg_mape<min_mape):\n",
    "          min_mape=avg_mape\n",
    "          min_list=[alpha, beta,lag,layer, unit, lr, epoch,rdo,diff_label]\n",
    "          best_data=scaled_data\n",
    "          best_e_data=scaled_e_data\n",
    "        print ('MIN MAPE:',min_mape);\n",
    "        print ('Best Hyperparams:',min_list);\n",
    "\n",
    "\n",
    "  #save lowest error and best hyperparameters to file\n",
    "  with open('output_hyperparameters/'+attack_title.replace('/','_')+'.txt', 'w') as f:\n",
    "      f.write('MAPE: '+str(min_mape))\n",
    "      f.write('\\nHyperparams: '+str(min_list))\n",
    "      f.close()\n",
    "\n",
    "  #optional forecast for the future trend using the best hyperparameters found for this attack type (disabled here)\n",
    "  forecast_it=False\n",
    "  if forecast_it:\n",
    "    for a in range(1,4):\n",
    "      print('seed:',a)\n",
    "      seed(a)\n",
    "      tf.random.set_seed(a)   \n",
    "      forecast(best_data,best_e_data, model,min_list,a, attack_title)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cc74c66458e8e490682e0d3377408d1cccb8fafc390b019bd054f05d2ad2aafd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
