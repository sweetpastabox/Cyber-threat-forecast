{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math\n",
    "from numpy import split\n",
    "import random\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import numpy\n",
    "from tqdm import tqdm_notebook\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from numpy import array\n",
    "#from itertools import product\n",
    "from pandas import read_csv\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from numpy.random import seed\n",
    "from scipy import stats\n",
    "from IPython.display import clear_output \n",
    "import statistics\n",
    "import keras.backend as K\n",
    "\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "\n",
    "def maximum(a, b):\n",
    "      \n",
    "    if a >= b:\n",
    "        return a\n",
    "    else:\n",
    "        return b\n",
    "\n",
    "def minimum(a, b):\n",
    "      \n",
    "    if a <= b:\n",
    "        return a\n",
    "    else:\n",
    "        return b\n",
    "\n",
    "#for parsing the units, this function removes the comma and replaces it with dash to avoid splitting the units (as different hyperparameters) when building the hyperparameters list\n",
    "def removeCommaInUnits(HP):\n",
    "  units= HP[HP.index('['):HP.index(']')+1]\n",
    "  if ',' in units:\n",
    "    units = units.replace(',','-')\n",
    "  HP=HP.replace(HP[HP.index('['):HP.index(']')+1],units)\n",
    "  return HP\n",
    "\n",
    "#this function puts back the comma in its place as it is needed while building our model\n",
    "def putCommaInUnitsList(HP):\n",
    "  return [i.replace('-',',') for i in HP] \n",
    "\n",
    "\n",
    "\n",
    "def exponential_smoothing(series, alpha):\n",
    "\n",
    "    result = [series[0]] # first value is same as series\n",
    "    for n in range(1, len(series)):\n",
    "        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n",
    "    return result\n",
    "  \n",
    "def plot_exponential_smoothing(series, alphas):\n",
    " \n",
    "    plt.figure(figsize=(17, 8))\n",
    "    for alpha in alphas:\n",
    "        plt.plot(exponential_smoothing(series, alpha), label=\"Alpha {}\".format(alpha))\n",
    "    plt.plot(series.values, \"c\", label = \"Actual\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.axis('tight')\n",
    "    plt.title(\"Exponential Smoothing - \"+attack)\n",
    "    plt.grid(True);\n",
    "\n",
    "def double_exponential_smoothing(series, alpha, beta):\n",
    "\n",
    "    result = [series[0]]\n",
    "    for n in range(1, len(series)+1):\n",
    "        if n == 1:\n",
    "            level, trend = series[0], series[1] - series[0]\n",
    "        if n >= len(series): # forecasting\n",
    "            value = result[-1]\n",
    "        else:\n",
    "            value = series[n]\n",
    "        last_level, level = level, alpha * value + (1 - alpha) * (level + trend)\n",
    "        trend = beta * (level - last_level) + (1 - beta) * trend\n",
    "        result.append(level + trend)\n",
    "    return result\n",
    "\n",
    "def plot_double_exponential_smoothing(series, alphas, betas):\n",
    "     \n",
    "    plt.figure(figsize=(17, 8))\n",
    "    for alpha in alphas:\n",
    "        for beta in betas:\n",
    "            plt.plot(double_exponential_smoothing(series, alpha, beta), label=\"Alpha {}, beta {}\".format(alpha, beta))\n",
    "    plt.plot(series.values, label = \"Actual\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.axis('tight')\n",
    "    plt.title(\"Double Exponential Smoothing - \"+attack)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#stationarity test\n",
    "def tsplot(y, lags=None, figsize=(12, 7), syle='bmh'):\n",
    "    \n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "        \n",
    "    with plt.style.context(style='bmh'):\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        layout = (2,2)\n",
    "        ts_ax = plt.subplot2grid(layout, (0,0), colspan=2)\n",
    "        acf_ax = plt.subplot2grid(layout, (1,0))\n",
    "        pacf_ax = plt.subplot2grid(layout, (1,1))\n",
    "        \n",
    "        y.plot(ax=ts_ax)\n",
    "        p_value = sm.tsa.stattools.adfuller(y)[1]\n",
    "        ts_ax.set_title(attack_title+' - Time Series Analysis Plots\\n Dickey-Fuller: p={0:.5f}'.format(p_value))\n",
    "        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n",
    "        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    " \n",
    "# convert history into inputs and outputs\n",
    "def to_supervised(train, n_input, n_out=1):\n",
    "    # flatten data\n",
    "    data = train #modified\n",
    "\n",
    "    X, y = list(), list()\n",
    "    in_start = 0\n",
    "    # step over the entire history one time step at a time\n",
    "    for _ in range(len(data)):\n",
    "        # define the end of the input sequence\n",
    "        in_end = in_start + n_input\n",
    "        out_end = in_end + n_out\n",
    "        # ensure we have enough data for this instance\n",
    "        if out_end <= len(data):\n",
    "            X.append(data[in_start:in_end, :])\n",
    "            y.append(data[in_end:out_end, 0])\n",
    "        # move along one time step\n",
    "        in_start += 1\n",
    "    return array(X), array(y)\n",
    " \n",
    "\n",
    "class BayesianLSTM(LSTM): #inherits LSTM but set training=True which keeps dropout on during test time.\n",
    "  def call(self, inputs):\n",
    "    return super().call(inputs, training=True)\n",
    "\n",
    "# train the model using Bayesian LSTM\n",
    "def build_model(train, test, n_input, layer, unit, epoch, lr,rdo):\n",
    "    # prepare data\n",
    "    train_x, train_y = to_supervised(train, n_input)\n",
    "    test_x, test_y=to_supervised(test,n_input)\n",
    "\n",
    "    # define parameters\n",
    "    verbose, epochs, batch_size = 0,epoch, 8\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    if layer==1:\n",
    "        model.add(BayesianLSTM(unit[0], input_shape=(n_input, 1),recurrent_dropout=rdo, activation = 'relu'))\n",
    "        model.add(RepeatVector(1))\n",
    "        model.add(BayesianLSTM(unit[0], activation = 'relu', return_sequences=True, recurrent_dropout=rdo))\n",
    "        model.add(TimeDistributed(Dense(100, activation = 'relu')))\n",
    "        model.add(TimeDistributed(Dense(1)))\n",
    "    elif layer==2:\n",
    "        model.add(BayesianLSTM(unit[0], activation = 'relu', input_shape=(n_input, 1),return_sequences=True,recurrent_dropout=rdo))\n",
    "        model.add(BayesianLSTM(unit[1], activation = 'relu' ,recurrent_dropout=rdo))\n",
    "        model.add(RepeatVector(1))\n",
    "        model.add(BayesianLSTM(unit[1], activation = 'relu', return_sequences=True,recurrent_dropout=rdo))\n",
    "        model.add(BayesianLSTM(unit[0], activation = 'relu', return_sequences=True,recurrent_dropout=rdo))\n",
    "        model.add(TimeDistributed(Dense(100, activation = 'relu')))\n",
    "        model.add(TimeDistributed(Dense(1)))\n",
    "    elif layer==3:\n",
    "        model.add(BayesianLSTM(unit[0], activation = 'relu', input_shape=(n_input, 1),return_sequences=True,recurrent_dropout=rdo))\n",
    "        model.add(BayesianLSTM(unit[1], activation = 'relu' ,return_sequences=True,recurrent_dropout=rdo))\n",
    "        model.add(BayesianLSTM(unit[2], activation = 'relu' ,recurrent_dropout=rdo))\n",
    "        model.add(RepeatVector(1))\n",
    "        model.add(BayesianLSTM(unit[2], activation = 'relu', return_sequences=True,recurrent_dropout=rdo))\n",
    "        model.add(BayesianLSTM(unit[1], activation = 'relu', return_sequences=True,recurrent_dropout=rdo))\n",
    "        model.add(BayesianLSTM(unit[0], activation = 'relu', return_sequences=True,recurrent_dropout=rdo))\n",
    "        model.add(TimeDistributed(Dense(100, activation = 'relu')))\n",
    "        model.add(TimeDistributed(Dense(1)))\n",
    "          \n",
    "\n",
    "    opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=opt, loss='mse', metrics=['mape'])\n",
    "    \n",
    "    #if the loss does not improve after 30 epochs, stop anyway (early stopping)\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=30)\n",
    "\n",
    "    # fit network\n",
    "    train_history= model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, validation_data=(test_x, test_y),verbose=verbose,callbacks=[es])\n",
    "    \n",
    "    loss = train_history.history['loss']\n",
    "    val_loss = train_history.history['val_loss']\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "#forecasts the future trend of a single attack 3 years in advance\n",
    "def forecast(data, e_data, smoothed, model, min_list,seed, attack_label):\n",
    "    \n",
    "    #get best model's hyperparameters\n",
    "    n_input=min_list[2]\n",
    "    layer=min_list[3]\n",
    "    unit=min_list[4]\n",
    "    lr=min_list[5]\n",
    "    epoch=min_list[6]\n",
    "    rdo=min_list[7]\n",
    "\n",
    "    #build the model using the best hyperparameters using the whole data\n",
    "    model = build_model(e_data, e_data, n_input, layer, unit, epoch, lr,rdo)\n",
    "\n",
    "    #Bayesian LSTM performs multiple iterations to create a prediction distribution (i.e., mean and variance)\n",
    "    n_iterations=10\n",
    "    predictions_2d=list()\n",
    "    for r in range(0,n_iterations):\n",
    "\n",
    "      predictions = list()\n",
    "      predictions.append(e_data[-1,0]) #for plotting purpose\n",
    "      #look back in test data\n",
    "      history=e_data[-n_input:,]\n",
    "\n",
    "      #predicts 36 months using sliding window\n",
    "      for i in range(36):\n",
    "        f_hat=model.predict(history.reshape(1,history.shape[0],history.shape[1]))\n",
    "        history=numpy.append(history, f_hat[0][0])\n",
    "        history=history.reshape(history.shape[0],1)\n",
    "        history=history[1:,]#move the sliding window 1 step\n",
    "        predictions.append(f_hat[0][0][0])\n",
    "      predictions_2d.append(predictions)\n",
    "\n",
    "    predictions_2d = array(predictions_2d)\n",
    "    predictions_mean=numpy.mean(predictions_2d,axis=0)#mean\n",
    "    predictions_var=numpy.var(predictions_2d,axis=0)#variance\n",
    "    std_predictions=numpy.std(predictions_2d,axis=0)#standard deviation\n",
    "    z=1.96\n",
    "    confidence_95=z*std_predictions/math.sqrt(len(predictions_mean))# 95% confidence interval\n",
    "\n",
    "    #calculate the height of data on the plot (plot range) to disable variance/95% conf.interval plot if too large or too small.\n",
    "    upper=maximum(numpy.amax(predictions_mean),numpy.amax(data))\n",
    "    lower=minimum(numpy.amin(predictions_mean),numpy.amin(data))\n",
    "    p_range=upper-lower\n",
    "    var_plot=True\n",
    "    max_var=0\n",
    "    for v in predictions_var:\n",
    "      if v>p_range:\n",
    "        var_plot=False\n",
    "      if v>max_var:\n",
    "        max_var=v\n",
    "    if max_var*30<p_range:\n",
    "      var_plot=False\n",
    "\n",
    "    conf_plot=True\n",
    "    max_conf=0\n",
    "    for conf in confidence_95:\n",
    "      if conf>p_range:\n",
    "        conf_plot=False\n",
    "      if conf>max_conf:\n",
    "        max_conf=conf\n",
    "    if max_conf*30<p_range:\n",
    "      conf_plot=False \n",
    "\n",
    "\n",
    "    print(e_data)\n",
    "    print(predictions_mean)\n",
    "    \n",
    "    #Plot the forecast\n",
    "    x=['2012', '2013','2014', '2015', '2016', '2017', '2018','2019', '2020', '2021','2022','2023','2024','2025']\n",
    "  #\tpyplot.plot(L,test[-n_input:, 0],'b-',label='Actual')\n",
    "    pyplot.plot(range(len(data)),data,'--', color=\"blue\",label='Data')\n",
    "    pyplot.plot(range(len(data), len(data)+len(predictions_mean)),predictions_mean,'--', color=\"red\",label='Prediction')\n",
    "    if conf_plot:\n",
    "      pyplot.fill_between(range(len(data), len(data)+len(predictions_mean)),(predictions_mean.reshape(-1)- (confidence_95.reshape(-1))), (predictions_mean.reshape(-1)+confidence_95.reshape(-1)), color='green', alpha=0.2,label='95% Confidence')\n",
    "    if var_plot:\n",
    "      pyplot.fill_between(range(len(data), len(data)+len(predictions_mean)),(predictions_mean.reshape(-1)- (predictions_var.reshape(-1))), (predictions_mean.reshape(-1)+predictions_var.reshape(-1)), color='green', alpha=0.1,label='Variance')\n",
    "    plt.xticks([6,18,30,42,54,66,78,90,102,114,126,138,150,162], x) # positions of years on x axis\n",
    "\n",
    "    plt.ylabel(\"No of incidents\",fontsize=15)\n",
    "    plt.yticks(fontsize=13)\n",
    "    plt.legend(loc=\"best\",prop={'size': 11})\n",
    "    plt.axis('tight')\n",
    "    plt.grid(True)\n",
    "    pyplot.xticks(rotation=90,fontsize=13)\n",
    "    pyplot.title(attack_label+' (U)', y=1.03,fontsize=18)\n",
    "     \n",
    "    #save and show the forecast\n",
    "    images_dir = 'output_forecast/'\n",
    "    plt.savefig(f\"{images_dir}/\"+attack_label.replace('/','_')+str(seed)+\".png\", bbox_inches = \"tight\")#the name of the file includes the seed\n",
    "    pyplot.show() \n",
    "\n",
    "\n",
    "    #write the data and forecast\n",
    "    data=data.ravel().tolist()\n",
    "    predictions_mean=predictions_mean.tolist()\n",
    "\n",
    "    files_dir = 'output_forecast/'\n",
    "    with open(files_dir+attack_title.replace('/','_')+'_data_'+str(seed)+'.txt', 'w') as f:#the name of the file includes the seed\n",
    "      f.write('Data: '+str(data))\n",
    "      f.write('\\nPredictions: '+str(predictions_mean))\n",
    "      data.extend(predictions_mean)\n",
    "      f.write('\\nData and Predictions: '+str(data))\n",
    "      f.write('\\n95_Confidence: '+str(confidence_95.tolist()))\n",
    "      f.write('\\nVariance: '+str(predictions_var.tolist()))\n",
    "      f.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#read input data\n",
    "data = read_csv('input_data/T-HMGDN-F-0711-0322.csv', header=0)\n",
    "attacks=data.columns.values.tolist()\n",
    "raw_values = data.values\n",
    "avg_data=0\n",
    "\n",
    "#name of attack columns (in the dataset) to be forecasted. \"All\" refers to the number of attacks in all countries combined \n",
    "attacks=['DDoS-ALL','Ransomware-ALL', 'Account Hijacking-ALL', 'Zero-day-ALL', 'Malware-ALL',\n",
    "         'Brute Force Attack-ALL', 'Botnet-ALL','Mentions-MITM', 'Mentions-Dropper', 'Mentions-Adversarial Attack', 'Mentions-Deepfake',\n",
    "          'Mentions-Supply Chain','Mentions-IoT Device Attack']\n",
    "done=[] #can be manually appended with already forecasted attacks to avoid repeating the process\n",
    "\n",
    "\n",
    "#iterate over all attacks to forecast their trend\n",
    "for attack in attacks:\n",
    "\n",
    "  #clear_output()\n",
    "  if attack in done:\n",
    "    continue\n",
    "  if not 'ALL' in attack and not 'Mentions' in attack:\n",
    "    continue;\n",
    "  if 'Mentions-' in attack:\n",
    "    attack_title=attack[attack.index('-')+1:]\n",
    "  else:\n",
    "    attack_title=attack[0:attack.rindex('-')]\n",
    "\n",
    "\n",
    "  if len(attack_title)>18:\n",
    "    attack_title=attack_title.rsplit(' ', 1)[0]\n",
    "\n",
    "\n",
    "\n",
    "  best_data=None\n",
    "  best_e_data=None\n",
    "\n",
    "\n",
    "  # get the optimal model hyperparameters from file\n",
    "  files_dir = 'output_hyperparameters/'\n",
    "  with open(f\"{files_dir}/\"+attack_title.replace('/','_')+'.txt') as f:\n",
    "      lines = f.readlines()\n",
    "      f.close()\n",
    "  HP_str= lines[1]\n",
    "  HP_str=HP_str[HP_str.index('[')+1:-1]\n",
    "  HP_str=removeCommaInUnits(HP_str)\n",
    "  HP=HP_str.split(', ')\n",
    "  HP=putCommaInUnitsList(HP)\n",
    "  print('HP:',HP)\n",
    "\n",
    "\n",
    "  min_mape=999999999\n",
    "  min_list=[]\n",
    "  model=None\n",
    "  c=1\n",
    "\n",
    "\n",
    "  #read smoothing constants\n",
    "  alpha=float(HP[0])\n",
    "  beta=float(HP[1])\n",
    "  \n",
    "  #Double Exponential Smoothing\n",
    "  if not alpha==1 and not beta==1:\n",
    "    smoothed=double_exponential_smoothing(data[attack], alpha,beta)\n",
    "    smoothed=smoothed[0:-1] #thanks for the one additional forecast but no thanks,\n",
    "    plt.plot(smoothed, label=\"Alpha {}, beta {}\".format(alpha, beta))\n",
    "    plt.title(\"Double Exponential Smoothing - \"+attack_title)\n",
    "    avg_data=sum(smoothed)/len(smoothed)\n",
    "  #Exponential Smoothing  \n",
    "  elif not alpha==1 and beta==1: \n",
    "    smoothed=exponential_smoothing(data[attack], alpha)\n",
    "    plt.plot(smoothed, label=\"Alpha {}\".format(alpha))\n",
    "    plt.title(\"Exponential Smoothing - \"+attack_title)\n",
    "    avg_data=sum(smoothed)/len(smoothed)\n",
    "  #no smoothing  \n",
    "  elif alpha==1 and beta==1:\n",
    "    smoothed=data[attack]\n",
    "    avg_data=smoothed.mean()\n",
    "\n",
    "  #plot the data\n",
    "  plt.plot(data[attack], label = \"Actual\")\n",
    "  plt.legend(loc=\"best\")\n",
    "  plt.axis('tight')\n",
    "  plt.grid(True)\n",
    "  plt.xlabel(\"Month\")\n",
    "  plt.ylabel(\"No of incidents\")\n",
    "  plt.show()\n",
    "\n",
    "  #stationarity test\n",
    "  tsplot(smoothed, lags=30)\n",
    "\n",
    "\n",
    "  smoothed_all=numpy.array([smoothed]).reshape(-1,1)\n",
    "  prepared_data=smoothed\n",
    "  scaled_data=numpy.array(prepared_data).reshape(-1,1)\n",
    "\n",
    "  #cut first part of the data if it is zeros (look for emerging index of the attack)\n",
    "  e_index=0 #emerging index\n",
    "  for i in range(len(scaled_data)-2):\n",
    "    if scaled_data[i,0]>0 and scaled_data[i+1,0]>0 and scaled_data[i+2,0]>0:\n",
    "        e_index=i\n",
    "        break\n",
    "  print('Emerging Index=',e_index)\n",
    "  scaled_e_data=scaled_data[e_index:]\n",
    "  print('Length of data after cut=',len(scaled_e_data))\n",
    "    \n",
    "  #Get model hyperparameters:\n",
    "  lag=int(HP[2])#lookback period\n",
    "  lr=float(HP[5])#learning rate\n",
    "  epoch=int(HP[6])# epochs\n",
    "  unit=list(map(int, HP[4].replace('[','').replace(']','').split(', '))) #units\n",
    "  layer=len(unit) #layers\n",
    "  rdo=float(HP[7]) #recurrent dropout\n",
    "  diff_label='not differenced' #no differencing\n",
    "\n",
    "\n",
    "  #build the set of hyperparameters based on the optimised model\n",
    "  hp_set=[alpha, beta,lag,layer, unit, lr, epoch,rdo,diff_label]\n",
    "  print('Hyperparameters:',hp_set)\n",
    "\n",
    "  #forecast the attack's future trend using 3 seeds 1-3\n",
    "  for a in range(1,4):\n",
    "    print('seed:',a)\n",
    "    seed(a)\n",
    "    tf.random.set_seed(a)   \n",
    "    forecast(scaled_data,scaled_e_data,smoothed_all, model,hp_set,a, attack_title)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cc74c66458e8e490682e0d3377408d1cccb8fafc390b019bd054f05d2ad2aafd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
